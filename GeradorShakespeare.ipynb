{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GeradorShakespeare.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj4bFFxiJ7ZA"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrlvLDrQKA25",
        "outputId": "119151aa-45f2-4b32-cca9-c1af367a2a34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMYxUlx8ReFG"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkLINle9PL1_",
        "outputId": "851a7354-e97d-4a77-c5fb-fcef2ccd3c0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwqtd2tJQxQl",
        "outputId": "a10cdd09-1b5d-4233-b4bd-ce464360a580",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryyJzyXiPdN2",
        "outputId": "0a45ae99-9b29-4c9b-d5fc-f8235a7ad4c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttNna-FlXKx-"
      },
      "source": [
        "Processando o texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu5towOWXixc"
      },
      "source": [
        "Vectorize o texto\n",
        "Antes do treinamento, precisamos mapear strings para uma representação numérica. Crie duas tabelas de pesquisa: uma mapeando caracteres para números e outra para números para caracteres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6GvMiL6XEr6"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtpjCKvfXm2M"
      },
      "source": [
        "Agora temos uma representação inteira para cada personagem. Observe que mapeamos o caractere como índices de 0 a len(unique) .\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UOk1FdNXNa-",
        "outputId": "c48ee8f1-77f9-41a0-fbaf-ec64c4d24a38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJkAh8PzXS9F",
        "outputId": "21b1f1e8-63b0-47f3-f408-81bfbb9ecb12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcEzJnKQXyZV"
      },
      "source": [
        "previsão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK5rQBQZXXZ8",
        "outputId": "edd1e68a-701f-49b8-bceb-43f36a71d6a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS9SHjdIX7lu"
      },
      "source": [
        "O método em batch nos permite converter facilmente esses caracteres individuais em sequências do tamanho desejado.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYwRoDWrX0qt",
        "outputId": "edd6ff44-c9f4-4fba-9e5e-9bc58524d41a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxf7wn9DYJM9"
      },
      "source": [
        "Para cada sequência, duplique e mude para formar o texto de entrada e de destino usando o método de map para aplicar uma função simples a cada lote:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kStiP2V0X-KE"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw86M-A5YP2d"
      },
      "source": [
        "Imprima os primeiros exemplos de valores de entrada e destino:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZP0aXjpYMB1",
        "outputId": "5c2d3311-c1da-4040-ebd6-7180d185610c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R89niJQfYwwl"
      },
      "source": [
        "Cada índice desses vetores é processado como uma etapa de tempo. Para a entrada na etapa de tempo 0, o modelo recebe o índice para \"F\" e tenta prever o índice para \"i\" como o próximo caractere. No próximo passo de tempo, ele faz a mesma coisa, mas o RNN considera o contexto da etapa anterior além do caractere de entrada atual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmnVoPGUYTF1",
        "outputId": "306b2af7-90e2-4cef-d3d1-fd1b9fb8e92d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "  print(\"Step {:4d}\".format(i))\n",
        "  print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "  print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54jwMXMMZVTE"
      },
      "source": [
        "**Cria treinamentos em lote**\n",
        "\n",
        "\n",
        "Usamos tf.data para dividir o texto em sequências gerenciáveis. Mas antes de alimentar esses dados no modelo, precisamos embaralhar os dados e empacotá-los em lotes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMxUR5M5YzvE",
        "outputId": "b0d7931e-a73a-4193-86df-fff07f322404",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw13vFgpZrsV"
      },
      "source": [
        "**Construa o modelo**\n",
        "\n",
        "Use tf.keras.Sequential para definir o modelo. Para este exemplo simples, três camadas são usadas para definir nosso modelo:\n",
        "\n",
        "tf.keras.layers.Embedding : A camada de entrada. Uma tabela de pesquisa treinável que mapeará os números de cada caractere para um vetor com dimensões embedding_dim ;\n",
        "tf.keras.layers.GRU : Um tipo de RNN com units=rnn_units tamanho units=rnn_units (você também pode usar uma camada LSTM aqui.)\n",
        "tf.keras.layers.Dense : A camada de saída, com saídas vocab_size ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eeq6CsHNZnLu"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbjzXqmwZz68"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3RvWgWWZ4h9"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size = len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq_vMsG4aPCO"
      },
      "source": [
        "Imagem das rede"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "816V5Qk4aTwM"
      },
      "source": [
        "Experimente o modelo\n",
        "Agora execute o modelo para ver se ele se comporta conforme o esperado.\n",
        "\n",
        "Primeiro verifique a forma da saída:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXJgclXHZ6WT",
        "outputId": "fd3a77c1-5742-433d-eb2e-8814460a6fbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOJifT1raZO-"
      },
      "source": [
        "No exemplo acima, o comprimento da sequência da entrada é 100 mas o modelo pode ser executado em entradas de qualquer comprimento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKQkhvTRaV4k",
        "outputId": "78d8123b-2493-4498-c6b0-88e2a222b15b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "782lbvyaae1M"
      },
      "source": [
        "Para obter previsões reais do modelo, precisamos obter uma amostra da distribuição de saída para obter índices de caracteres reais. Esta distribuição é definida pelos logits sobre o vocabulário dos caracteres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXiqE1ZgabIb"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FAu8gQGalTD"
      },
      "source": [
        "Isso nos dá, a cada passo de tempo, uma previsão do próximo índice de caractere:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGqNGJpmagxV",
        "outputId": "b6c847c5-0f60-48af-a913-8a00962da9e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([12, 38,  8, 35, 51, 57, 18, 15, 16, 61, 27, 22,  9, 52, 30, 26,  0,\n",
              "       53,  7, 53, 24, 63, 26,  3, 35,  0,  9, 63, 41, 50, 57, 42, 21, 27,\n",
              "       35,  8, 26, 18, 32, 36, 39,  4,  6, 50, 37, 41, 57, 13, 22, 33, 44,\n",
              "        1, 28, 18, 24, 12, 22,  6, 26, 24, 63, 39, 20, 44,  3, 59,  0, 63,\n",
              "       12,  2, 45, 14, 27, 43, 27, 30, 25, 32, 37, 24, 49, 39, 51, 38, 48,\n",
              "        4, 15, 60, 63, 12, 45,  6, 40, 58, 52, 56, 60, 63, 32, 42])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6RiIAFlarfb"
      },
      "source": [
        "Decodifique-os para ver o texto previsto por este modelo não treinado:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRhym_AMapvr",
        "outputId": "e01f663a-5b0e-4f94-8bd9-566748242e9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " ' seen him thus.\\n\\nMARCIUS:\\n\\nCOMINIUS:\\nThe shepherd knows not thunder from a tabour\\nMore than I know t'\n",
            "\n",
            "Next Char Predictions: \n",
            " '?Z.WmsFCDwOJ3nRN\\no-oLyN$W\\n3yclsdIOW.NFTXa&,lYcsAJUf PFL?J,NLyaHf$u\\ny?!gBOeORMTYLkamZj&Cvy?g,btnrvyTd'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYlUKmY8ay4b"
      },
      "source": [
        "**Treine o modelo**\n",
        "\n",
        "Neste ponto, o problema pode ser tratado como um problema de classificação padrão. Dado o estado RNN anterior e a entrada desta etapa de tempo, preveja a classe do próximo caractere.\n",
        "\n",
        "Anexe um otimizador e uma função de perda\n",
        "A função de perda tf.keras.losses.sparse_categorical_crossentropy padrão funciona neste caso porque é aplicada na última dimensão das previsões.\n",
        "\n",
        "Como nosso modelo retorna logits, precisamos definir o sinalizador from_logits ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNVK-5ssath8",
        "outputId": "c5a304cf-5b7a-411d-b11b-323655fa5824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.1750736\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0ZjHvLCbchs"
      },
      "source": [
        "Configure o procedimento de treinamento usando o método tf.keras.Model.compile . Usaremos tf.keras.optimizers.Adam com argumentos padrão e a função de perda."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TB8RHrObZg0"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO1L-bGsbg0r"
      },
      "source": [
        "**Configurar pontos de verificação**\n",
        "\n",
        "Use um tf.keras.callbacks.ModelCheckpoint para garantir que os pontos de verificação sejam salvos durante o treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXNqpfa7bf_V"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_ivZgWzboWz"
      },
      "source": [
        "**Execute o treinamento**\n",
        "\n",
        "Para manter o tempo de treinamento razoável, use 10 épocas para treinar o modelo. No Colab, defina o tempo de execução para GPU para um treinamento mais rápido."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IEhZvWHbmK8",
        "outputId": "f18e663c-7da8-4d37-e14c-227ff3ae1bfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        }
      },
      "source": [
        "EPOCHS=10\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 903s 5s/step - loss: 2.6196\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 864s 5s/step - loss: 1.9394\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 873s 5s/step - loss: 1.6749\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 871s 5s/step - loss: 1.5320\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 868s 5s/step - loss: 1.4468\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 861s 5s/step - loss: 1.3883\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 875s 5s/step - loss: 1.3434\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 866s 5s/step - loss: 1.3054\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 864s 5s/step - loss: 1.2702\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 859s 5s/step - loss: 1.2379\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOkR6QMhb-_0"
      },
      "source": [
        "**Gerar texto **\n",
        "\n",
        "Restaure o último ponto de verificação\n",
        "Para manter essa etapa de previsão simples, use um tamanho de lote de 1.\n",
        "\n",
        "Por causa da forma como o estado RNN é passado de timestep para timestep, o modelo só aceita um tamanho de lote fixo depois de construído.\n",
        "\n",
        "Para executar o modelo com um batch_size diferente, precisamos reconstruir o modelo e restaurar os pesos do ponto de verificação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUOB74Wdb78E",
        "outputId": "11b3f493-1059-4c94-d4cf-9cfa9f325826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXABwC-IcJWx"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQXawZivcLAN",
        "outputId": "9482fa51-e156-4ceb-c804-124e6eaa7a10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA6kGVAucPBD"
      },
      "source": [
        "**O loop de previsão **\n",
        "\n",
        "O seguinte bloco de código gera o texto:\n",
        "\n",
        "Ele começa escolhendo uma string inicial, inicializando o estado RNN e definindo o número de caracteres a serem gerados.\n",
        "\n",
        "Obtenha a distribuição de previsão do próximo caractere usando a string inicial e o estado RNN.\n",
        "\n",
        "Em seguida, use uma distribuição categórica para calcular o índice do caráter previsto. Use este caractere previsto como nossa próxima entrada para o modelo.\n",
        "\n",
        "O estado RNN retornado pelo modelo é realimentado no modelo para que agora tenha mais contexto, em vez de apenas um caractere. Depois de prever o próximo caractere, os estados RNN modificados são novamente realimentados no modelo, que é como ele aprende conforme obtém mais contexto dos caracteres preditos anteriormente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdbsoBV4cTj0"
      },
      "source": [
        "Imagem aqui"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um7sr_G4cZw6"
      },
      "source": [
        "Olhando para o texto gerado, você verá que o modelo sabe quando capitalizar, fazer parágrafos e imitar um vocabulário de escrita semelhante ao de Shakespeare. Com o pequeno número de épocas de treinamento, ainda não aprendeu a formar frases coerentes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se_Gb69xcUGv"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # We pass the predicted character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS7WpgnJcdJW",
        "outputId": "4e896c35-5912-4ef1-b53f-7a21f5831207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: I must go with me! were he hat\n",
            "The more, that I am so but beauty oath.\n",
            "\n",
            "JULIET:\n",
            "The matter that our chait of what I love\n",
            "Much money than shape, two stops\n",
            "That she had there be at them that I say?\n",
            "Tell you, pribry that be? speak you, speak be?\n",
            "Have bee babe the ground of it? Thus ballads, my mistress, which lies\n",
            "That wench it, that lives long, slaud,\n",
            "I'come not some on pault.\n",
            "\n",
            "LADY GREY:\n",
            "His special apparelling fall about\n",
            "So she and unreqoice and tenter by the war\n",
            "Than dies it with your poison's land's change,\n",
            "To thee favoury that as engrail.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "Welcome! she is your brother, unleantoming\n",
            "As this is it then appreated: pardon,\n",
            "I long ip us both; say you go't before 't:\n",
            "And that this conference 'scape,\n",
            "And the racime he comes company,\n",
            "As shall cheekself, thou hadst not so?\n",
            "\n",
            "TRANIO:\n",
            "And to lenging beating not? this is impedience\n",
            "phased for my pittenly guides them not,\n",
            "For one their beaves him, let not the basif eas! What will our cast wrongs' black land.\n",
            "What's the god smother\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JjpWnHOcidD"
      },
      "source": [
        "A coisa mais fácil que você pode fazer para melhorar os resultados é treiná-lo por mais tempo (tente EPOCHS=30 ).\n",
        "\n",
        "Você também pode experimentar uma sequência de início diferente ou tentar adicionar outra camada RNN para melhorar a precisão do modelo ou ajustar o parâmetro de temperatura para gerar previsões mais ou menos aleatórias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f0aQexrchqT"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BYtXIsCSfbN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}